{
    "Subtopic": "Neural Networks - Basic Concepts",
    "Questions": [
        {
            "name": "What is gradient vanishing and gradient exploding problem?",
            "answer": [
                "When we back propagate to update the weights, we update the weights by subtract old weights with learning rate and gradient.",
                "In formula, new_weight = old_weight - learning_rate * gradient, where the gradient is the partial derivative of the loss function with respect to the weight.",
                "If the gradient is initialized too small, as number of hidden layers grows, gradient becomes very small and weights will hardly change.",
                "This will hamper the learning process, which is a gradient vanishing problem.",
                "If the gradient is initialized too large, individual derivatives are large, by chain rule, the final gradient(derivative) becomes very large and weights will change dramatically.",
                "This will also hamper the learning process, which is a gradient exploding problem."
            ]
        },
        {
            "name": "Can we initialize all weights to 0?",
            "answer": [
                "No, if we initialize all weights to 0, all neurons will have the same output and same gradient, which means all neurons will be updated in the same way.",
                "This will make the neural network useless."
            ]
        },
        {
            "name": "How to initialize weights?",
            "answer": [
                "We can initialize weights randomly.",
                "For example, we can initialize weights from a normal distribution with mean 0 and standard deviation 1 (Xavier initialization).",
                "We can also initialize weights from a uniform distribution between -1 and 1.",
                "The reason is the mean of the activation function is 0 and the variance stays the same across different layers."
            ]
        },
        {
            "name": "How to prevent gradient vanishing and gradient exploding?",
            "answer": [
                "Using different activation functions for vanishing gradient problem",
                "Using different weight initialization methods",
                "Using gradient clipping with a threshold for gradient exploding problem",
                "Using differnet optimization and learning rate"
            ]           
        }
    ]
}